{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2ad6824d-f12e-4afe-90ca-df57bc6aaf18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: requests in c:\\users\\szt10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\szt10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\szt10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\szt10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\szt10\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests) (2024.8.30)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63af3597-efc5-4c3f-9ea9-963782e3437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching SIGCHI conferences...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 230\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSaved to conferences.csv and conferences_normalized.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    229\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 230\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[1], line 197\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmain\u001b[39m():\n\u001b[0;32m    196\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching SIGCHI conferences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 197\u001b[0m     sigchi_confs \u001b[38;5;241m=\u001b[39m \u001b[43mfetch_sigchi_conferences\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(sigchi_confs)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m SIGCHI conferences.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    200\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFetching SIGPLAN conferences...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[1], line 139\u001b[0m, in \u001b[0;36mfetch_sigchi_conferences\u001b[1;34m()\u001b[0m\n\u001b[0;32m    136\u001b[0m     seen_links\u001b[38;5;241m.\u001b[39madd(href)\n\u001b[0;32m    137\u001b[0m     full_link \u001b[38;5;241m=\u001b[39m href \u001b[38;5;28;01mif\u001b[39;00m href\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m urljoin(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://sigchi.org\u001b[39m\u001b[38;5;124m\"\u001b[39m, href)\n\u001b[1;32m--> 139\u001b[0m     year, start_date, end_date, location \u001b[38;5;241m=\u001b[39m fetch_page_details(full_link)\n\u001b[0;32m    141\u001b[0m     conferences\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    142\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSIGCHI\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    143\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m\"\u001b[39m: text,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlocation\u001b[39m\u001b[38;5;124m\"\u001b[39m: location\n\u001b[0;32m    149\u001b[0m     })\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m conferences\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import re\n",
    "from urllib.parse import urljoin\n",
    "from datetime import datetime\n",
    "\n",
    "# Small utilities for normalization\n",
    "MONTHS = {m.lower(): i for i, m in enumerate(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'], start=1)}\n",
    "\n",
    "# Helper functions for extraction and fetching\n",
    "def extract_year(text):\n",
    "    match = re.search(r'20\\d{2}', text)\n",
    "    return match.group(0) if match else \"\"\n",
    "\n",
    "def parse_date_to_iso(month_name: str, day: int, year: int):\n",
    "    try:\n",
    "        # Normalize month name (accept full or abbreviated)\n",
    "        month_num = datetime.strptime(month_name[:3], '%b').month\n",
    "        return datetime(year, month_num, day).date().isoformat()\n",
    "    except Exception:\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def parse_date_range(text: str):\n",
    "    \"\"\"Try several regex patterns to extract start and end dates and return ISO strings.\n",
    "\n",
    "    Returns (start_iso, end_iso)\n",
    "    \"\"\"\n",
    "    if not text:\n",
    "        return \"\", \"\"\n",
    "\n",
    "    # Patterns to match different common date ranges\n",
    "    # Examples: \"July 10-12, 2024\" or \"July 10, 2024 - July 12, 2024\" or \"10 July 2024\"\n",
    "    patterns = [\n",
    "        # July 10-12, 2024\n",
    "        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+(\\d{1,2})-(\\d{1,2}),?\\s+(20\\d{2})',\n",
    "        # July 10, 2024 - July 12, 2024\n",
    "        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+(\\d{1,2}),?\\s+(20\\d{2})\\s*[-–—]\\s*(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s*(\\d{1,2}),?\\s*(20\\d{2})',\n",
    "        # 10 July 2024\n",
    "        r'(\\d{1,2})\\s+(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+(20\\d{2})',\n",
    "        # July 10, 2024\n",
    "        r'(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)[a-z]*\\.?\\s+(\\d{1,2}),?\\s+(20\\d{2})'\n",
    "    ]\n",
    "\n",
    "    for pat in patterns:\n",
    "        m = re.search(pat, text, flags=re.IGNORECASE)\n",
    "        if m:\n",
    "            try:\n",
    "                if len(m.groups()) == 4:\n",
    "                    # pattern 1: Mon Day-Day, Year\n",
    "                    mon = m.group(1)\n",
    "                    day1 = int(m.group(2))\n",
    "                    day2 = int(m.group(3))\n",
    "                    year = int(m.group(4))\n",
    "                    start = parse_date_to_iso(mon, day1, year)\n",
    "                    end = parse_date_to_iso(mon, day2, year)\n",
    "                    return start, end\n",
    "                elif len(m.groups()) == 6:\n",
    "                    # pattern 2: Mon D, Y - Mon D, Y\n",
    "                    mon1 = m.group(1); d1 = int(m.group(2)); y1 = int(m.group(3))\n",
    "                    mon2 = m.group(4); d2 = int(m.group(5)); y2 = int(m.group(6))\n",
    "                    start = parse_date_to_iso(mon1, d1, y1)\n",
    "                    end = parse_date_to_iso(mon2, d2, y2)\n",
    "                    return start, end\n",
    "                elif len(m.groups()) == 3:\n",
    "                    # pattern 3 or 4: Day Month Year  OR Month Day, Year\n",
    "                    g1 = m.group(1); g2 = m.group(2); g3 = m.group(3)\n",
    "                    # decide which is day vs month\n",
    "                    if g1.isdigit():\n",
    "                        day = int(g1); mon = g2; year = int(g3)\n",
    "                    else:\n",
    "                        mon = g1; day = int(g2); year = int(g3)\n",
    "                    iso = parse_date_to_iso(mon, day, year)\n",
    "                    return iso, \"\"\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "    return \"\", \"\"\n",
    "\n",
    "\n",
    "def normalize_location(raw: str):\n",
    "    if not raw:\n",
    "        return \"\"\n",
    "    # remove labels and excessive whitespace, keep ',' separators\n",
    "    raw = re.sub(r'(?i)^(Location|Place|Where|Venue)[:\\s]*', '', raw).strip()\n",
    "    raw = re.sub(r'\\s*[-–—]\\s*', ', ', raw)\n",
    "    raw = re.sub(r'\\s{2,}', ' ', raw)\n",
    "    # remove trailing words like 'USA' leftover attached oddly\n",
    "    return raw.strip(', ').strip()\n",
    "\n",
    "def fetch_page_details(url):\n",
    "    # Fetch the conference page and extract year, start_date, location\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        if response.status_code != 200:\n",
    "            return \"\", \"\", \"\"\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        year = extract_year(url) or extract_year(soup.title.string if soup.title else \"\")\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "        start_date, end_date = parse_date_range(text)\n",
    "        # try common labeled location fields too\n",
    "        loc_match = re.search(r'(?i)(Location|Place|Where|Venue)[:\\s]+([^\\n,\\r]{3,200})', text)\n",
    "        location = normalize_location(loc_match.group(2)) if loc_match else \"\"\n",
    "        return year, start_date, end_date, location\n",
    "    except Exception:\n",
    "        return \"\", \"\", \"\", \"\"\n",
    "\n",
    "# SIGCHI\n",
    "def fetch_sigchi_conferences():\n",
    "    url = \"https://sigchi.org/conferences/\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    conferences = []\n",
    "    seen_links = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        text = a.get_text(strip=True)\n",
    "        href = a[\"href\"]\n",
    "\n",
    "        # Skip empty or too short links\n",
    "        if not text or len(text) < 3 or href in seen_links:\n",
    "            continue\n",
    "\n",
    "        # for use in eliminating links deemed as \"non-conference links\"\n",
    "        exclude_keywords = [\n",
    "            \"what is sigchi\", \"upcoming conference\", \"conference history\", \"publications\", \"ethics and conduct\", \"policies\", \"policy\",\n",
    "            \"cares\", \"voting history\", \"contact us\", \"membership\", \"executive committee\", \"all committees\", \"chapters\", \"awards\", \"guides\",\n",
    "            \"blog\", \"meetings\", \"announcements\", \"volunteer history\", \"open calls\", \"development fund\", \"digital library\", \"join us\",\n",
    "            \"calendar\", \"updates\", \"application forms\", \"programs app\", \"youtube\", \"home\", \"about\", \"news\", \"events\"\n",
    "        ]\n",
    "        if any(kw in text.lower() for kw in exclude_keywords):\n",
    "            continue\n",
    "\n",
    "        seen_links.add(href)\n",
    "        full_link = href if href.startswith(\"http\") else urljoin(\"https://sigchi.org\", href)\n",
    "\n",
    "        year, start_date, end_date, location = fetch_page_details(full_link)\n",
    "\n",
    "        conferences.append({\n",
    "            \"source\": \"SIGCHI\",\n",
    "            \"name\": text,\n",
    "            \"link\": full_link,\n",
    "            \"year\": year,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"location\": location\n",
    "        })\n",
    "\n",
    "    return conferences\n",
    "\n",
    "# SIGPLAN\n",
    "def fetch_sigplan_conferences():\n",
    "    url = \"https://sigplan.org/Conferences\"\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    conferences = []\n",
    "    seen_links = set()\n",
    "\n",
    "    for a in soup.find_all(\"a\", href=True):\n",
    "        text = a.get_text(strip=True)\n",
    "        href = a[\"href\"]\n",
    "\n",
    "        # Skip empty or very short links\n",
    "        if not text or len(text) < 3 or href in seen_links:\n",
    "            continue\n",
    "\n",
    "        # for use in eliminating links deemed as \"non-conference links\"\n",
    "        exclude_keywords = [\n",
    "            \"home\", \"about\", \"contact\", \"news\", \"conferences\", \"jobs\", \"awards\", \"opentoc\", \"sigplan\", \"research highlights\",\n",
    "            \"membership\", \"calendar\", \"organizers\"\n",
    "        ]\n",
    "        if any(kw in text.lower() for kw in exclude_keywords):\n",
    "            continue\n",
    "\n",
    "        seen_links.add(href)\n",
    "        full_link = href if href.startswith(\"http\") else urljoin(\"https://sigplan.org\", href)\n",
    "\n",
    "        year, start_date, end_date, location = fetch_page_details(full_link)\n",
    "\n",
    "        conferences.append({\n",
    "            \"source\": \"SIGPLAN\",\n",
    "            \"name\": text,\n",
    "            \"link\": full_link,\n",
    "            \"year\": year,\n",
    "            \"start_date\": start_date,\n",
    "            \"end_date\": end_date,\n",
    "            \"location\": location\n",
    "        })\n",
    "\n",
    "    return conferences\n",
    "\n",
    "def main():\n",
    "    print(\"Fetching SIGCHI conferences...\")\n",
    "    sigchi_confs = fetch_sigchi_conferences()\n",
    "    print(f\"Found {len(sigchi_confs)} SIGCHI conferences.\")\n",
    "\n",
    "    print(\"Fetching SIGPLAN conferences...\")\n",
    "    sigplan_confs = fetch_sigplan_conferences()\n",
    "    print(f\"Found {len(sigplan_confs)} SIGPLAN conferences.\")\n",
    "\n",
    "    all_confs = sigchi_confs + sigplan_confs\n",
    "    # Basic cleaning/normalization pass\n",
    "    def clean_name(n):\n",
    "        if not n:\n",
    "            return n\n",
    "        # remove year-like suffixes in name (e.g., 'Conference 2024')\n",
    "        n = re.sub(r'\\(?20\\d{2}\\)?', '', n).strip()\n",
    "        n = re.sub(r'\\s{2,}', ' ', n)\n",
    "        return n\n",
    "\n",
    "    for c in all_confs:\n",
    "        c['name'] = clean_name(c.get('name', ''))\n",
    "        c['location'] = normalize_location(c.get('location', ''))\n",
    "        # ensure year is four-digit or empty\n",
    "        y = c.get('year', '') or ''\n",
    "        c['year'] = y if re.match(r'^20\\d{2}$', str(y)) else ''\n",
    "\n",
    "    df = pd.DataFrame(all_confs)\n",
    "    # drop exact duplicates by name+year+link\n",
    "    df = df.drop_duplicates(subset=['name', 'year', 'link'])\n",
    "    # Save both raw and normalized outputs\n",
    "    df.to_csv(\"conferences.csv\", index=False)\n",
    "    df.to_csv(\"conferences_normalized.csv\", index=False)\n",
    "    print(\"\\nSaved to conferences.csv and conferences_normalized.csv\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
